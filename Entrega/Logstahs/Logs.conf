input {
  file {
    path => "/home/g6/reto/datos/scraper_metrics.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    tags => ["scraper_metrics"]
  }
}

filter {
  if "scraper_metrics" in [tags] {
    
    # --- 1. Separar Cabecera del Mensaje ---
    grok {
      match => { 
        "message" => "^%{TIMESTAMP_ISO8601:log_timestamp} - %{LOGLEVEL:log_level} - %{GREEDYDATA:raw_message}$" 
      }
    }

    # --- 2. Extraer datos segÃºn el tipo de log ---
    if [log_level] == "INFO" {
      grok {
        match => {
          "raw_message" => "REQUEST_URL:%{DATA:request_url} \| STATUS:%{INT:http_status:int} \| LATENCY:%{NUMBER:api_latency:float}s \| OFFSET:%{INT:search_offset:int}"
        }
      }
    } else if [log_level] == "ERROR" {
      grok {
        match => {
          "raw_message" => "(?:%{WORD:error_type} \| URL:%{DATA:failed_url} \| OFFSET:%{INT:failed_offset})|(?:%{WORD:error_type} \| URL:%{DATA:failed_url} \| ERROR:%{GREEDYDATA:error_details})"
        }
      }
    }

    # --- 3. Fecha real ---
    date {
      match => [ "log_timestamp", "yyyy-MM-dd HH:mm:ss" ]
      target => "@timestamp"
      timezone => "Europe/Madrid"
    }

    # --- 4. Limpiar campos intermedios ---
    mutate {
      remove_field => ["message", "raw_message", "host", "path"]
    }
  }
}

output {
  if "scraper_metrics" in [tags] {
    elasticsearch {
      hosts => ["https://192.199.1.53:9200","https://192.199.1.65:9200","https://192.199.1.66:9200"]
      index => "scraper_logs-%{+yyyy.MM.dd}"
      api_key => "JmdB2ZoBHVbb80fUHprF:jrOb-T2fpjnK5OHGZEJLOQ"
      ssl_certificate_authorities => ["/home/g6/reto/elasticsearch-9.2.1/config/certs/http_ca.crt"]
      ssl_verification_mode => "full"
      resurrect_delay => 60
      sniffing => false
    }

    #stdout { codec => rubydebug }
  }
}
